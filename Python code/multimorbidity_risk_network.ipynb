{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "306ea5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from hcuppy.elixhauser import ElixhauserEngine\n",
    "import json\n",
    "import networkx as nx\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import powerlaw\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "\n",
    "path = 'G:/eicu-crd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283bc10",
   "metadata": {},
   "source": [
    "# Identification of multimorbidity\n",
    "\n",
    "## 与MIMIC不同，eICU数据集更适合我们研究共病，因为患者的诊断往往由多个ICD代码组成。除了诊断之外，apachePredVar表中还列出了几种常见的合并症。这些疾病包括艾滋病、肝功能衰竭、肝硬化、糖尿病、免疫抑制、白血病、淋巴瘤和转移性癌症。此代码将利用diagnosis表和apachePredVar表构建eICU的共病。并在这里进行初步的分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cb0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import diagnosis and apachePredVar table\n",
    "completed_df = pd.read_csv(path + '/completed_data.csv')\n",
    "diagnosis_cols = ['patientunitstayid', 'diagnosisstring', 'icd9code']\n",
    "diagnosis_df = pd.read_csv(path + '/diagnosis.csv', usecols=diagnosis_cols)\n",
    "apacheprevar_cols = ['patientunitstayid', 'aids', 'hepaticfailure', 'cirrhosis', 'diabetes', \n",
    "                     'immunosuppression', 'leukemia', 'lymphoma', 'metastaticcancer']\n",
    "apachepredvar_df = pd.read_csv(path + '/apachePredVar.csv', usecols=apacheprevar_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c16ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 2535918/2535918 [03:22<00:00, 12527.65it/s]                                                 \n"
     ]
    }
   ],
   "source": [
    "# Define common comorbidities that are included in apachepredvar_df\n",
    "comorbidities_columns = ['aids', 'hepaticfailure', 'cirrhosis', 'diabetes', 'immunosuppression', \n",
    "                         'leukemia', 'lymphoma', 'metastaticcancer']\n",
    "comorbidities_icd9_map = {\n",
    "    'aids': '042',\n",
    "    'hepaticfailure': '570.0',\n",
    "    'cirrhosis': '571.2',\n",
    "    'diabetes': '250.0',\n",
    "    'immunosuppression': '279.9',\n",
    "    'leukemia': '208.9',\n",
    "    'lymphoma': '202.9',\n",
    "    'metastaticcancer': '196.9'\n",
    "}\n",
    "# Merge two dataframes\n",
    "multimorbidity = apachepredvar_df[['patientunitstayid'] + comorbidities_columns].merge(diagnosis_df, on='patientunitstayid', how='left')\n",
    "\n",
    "tqdm.pandas(bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')\n",
    "# Extract multimorbidity patterns（string and icd9）\n",
    "def extract_comorbidities(row):\n",
    "    comorbidities = [col for col in comorbidities_columns if row[col] == 1]\n",
    "    # Get comorbidities (string)\n",
    "    comorbidities_string = '|'.join(comorbidities)\n",
    "    # Get comorbidities (icd9)\n",
    "    comorbidities_icd9 = ', '.join([comorbidities_icd9_map[col] for col in comorbidities])\n",
    "    diagnosis_result = str(row['diagnosisstring']) if pd.notna(row['diagnosisstring']) else ''\n",
    "    diagnosis_icd9_result = str(row['icd9code']) if pd.notna(row['icd9code']) else ''\n",
    "    \n",
    "    # Improved logic\n",
    "    if diagnosis_result:\n",
    "        final_string = diagnosis_result + ('|' + comorbidities_string if comorbidities_string else '')\n",
    "    else:\n",
    "        final_string = comorbidities_string\n",
    "\n",
    "    if diagnosis_icd9_result:\n",
    "        final_icd9 = diagnosis_icd9_result + (', ' + comorbidities_icd9 if comorbidities_icd9 else '')\n",
    "    else:\n",
    "        final_icd9 = comorbidities_icd9\n",
    "\n",
    "    return pd.Series([final_string, final_icd9], index=['multimorbidity_patterns_string', 'multimorbidity_patterns_icd9'])\n",
    "\n",
    "# Use progress_apply instead of apply, for progress bar functionality\n",
    "multimorbidity[['multimorbidity_patterns_string', 'multimorbidity_patterns_icd9']] = multimorbidity.progress_apply(extract_comorbidities, axis=1)\n",
    "## Delete rows that have only one diseased row line, a disease, which removes 'multimorbidity_patterns' column does not contain '|' line\n",
    "# multimorbidity = multimorbidity[multimorbidity['multimorbidity_patterns'].str.contains('|', regex=False)]\n",
    "multimorbidity = multimorbidity[['patientunitstayid', 'multimorbidity_patterns_string', 'multimorbidity_patterns_icd9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ad4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimorbidity.replace('', np.nan, inplace=True)\n",
    "multimorbidity.dropna(subset=['multimorbidity_patterns_string', 'multimorbidity_patterns_icd9'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51b7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by patientunitstayid and aggregate by concatenating diagnosis results\n",
    "aggregated_multimorbidity = multimorbidity.groupby('patientunitstayid').agg({\n",
    "    'multimorbidity_patterns_string': lambda x: '|'.join(filter(None, x.astype(str))),\n",
    "    'multimorbidity_patterns_icd9': lambda x: ', '.join(filter(None, x.astype(str)))\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87094319",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_diseases = set()\n",
    "for patterns in aggregated_multimorbidity['multimorbidity_patterns_string']:\n",
    "    diseases = patterns.split('|')\n",
    "    for disease in diseases:\n",
    "        unique_diseases.add(disease.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678719e",
   "metadata": {},
   "source": [
    "## 统计所有疾病，通过LLM，gpt-4来标注其是否为慢性病以及对应的icd10code。prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64acd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_diseases = pd.DataFrame(list(unique_diseases), columns=['Disease'])\n",
    "# df_unique_diseases.to_excel(path + '_unique_diseases.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd7e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_of_chronic_diseases = pd.read_excel('G:/eicu-crd_unique_diseases.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa120a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_9268/2208086499.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  aggregated_multimorbidity = aggregated_multimorbidity[~aggregated_multimorbidity.applymap(lambda x: x == '' or pd.isna(x)).any(axis=1)]\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_9268/2208086499.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  aggregated_multimorbidity = aggregated_multimorbidity[aggregated_multimorbidity.applymap(lambda x: ',' in str(x)).any(axis=1)]\n"
     ]
    }
   ],
   "source": [
    "# Filter out the data belonging to chronic diseases\n",
    "chronic_diseases = dic_of_chronic_diseases[dic_of_chronic_diseases['ChronicDisease'] == 1]\n",
    "\n",
    "# Create a mapping from disease names to their ICD-10 codes\n",
    "disease_to_icd10 = dict(zip(chronic_diseases['Disease'], chronic_diseases['ICD-10 Code']))\n",
    "\n",
    "def filter_and_map(diseases_str):\n",
    "    diseases_list = diseases_str.split('|')\n",
    "    filtered_diseases = [disease_to_icd10[disease] for disease in diseases_list if disease in disease_to_icd10]\n",
    "    # Remove duplicate ICD-10 codes using set and then convert back to list\n",
    "    unique_filtered_diseases = list(set(filtered_diseases))\n",
    "    return ', '.join(unique_filtered_diseases)\n",
    "\n",
    "aggregated_multimorbidity['multimorbidity_icd10'] = aggregated_multimorbidity['multimorbidity_patterns_string'].apply(filter_and_map)\n",
    "\n",
    "# Remove the multimorbidity_patterns_icd9 column\n",
    "aggregated_multimorbidity = aggregated_multimorbidity.drop(columns=['multimorbidity_patterns_icd9'])\n",
    "aggregated_multimorbidity = aggregated_multimorbidity.drop(columns=['multimorbidity_patterns_string'])\n",
    "aggregated_multimorbidity['multimorbidity_icd10'] = aggregated_multimorbidity['multimorbidity_icd10'].str.replace('ICD-10 varies, ', '').str.replace(', ICD-10 varies', '')\n",
    "aggregated_multimorbidity = aggregated_multimorbidity[~aggregated_multimorbidity.applymap(lambda x: x == '' or pd.isna(x)).any(axis=1)]\n",
    "aggregated_multimorbidity = aggregated_multimorbidity[aggregated_multimorbidity.applymap(lambda x: ',' in str(x)).any(axis=1)]\n",
    "aggregated_multimorbidity.to_csv(path + '/multimorbidity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751ba47",
   "metadata": {},
   "source": [
    "# Exploration of the distribution of multimorbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fcf149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the curve of frequency distribution.\n",
    "multimorbidity_pattern_counts = aggregated_multimorbidity['multimorbidity_icd10'].value_counts()\n",
    "values = multimorbidity_pattern_counts.values\n",
    "# Calculate the number of multimorbidity patterns for each occurrence\n",
    "frequency_counts = np.bincount(values)\n",
    "# Get the index of the nonzero element, which is the number of occurrences of each element\n",
    "non_zero_indices = np.nonzero(frequency_counts)[0]\n",
    "\n",
    "output_data = pd.DataFrame({\n",
    "    'Frequency': non_zero_indices,\n",
    "    'Number_of_Multimorbidity': frequency_counts[non_zero_indices]\n",
    "})\n",
    "# Output data points to a CSV file\n",
    "output_path = 'G:/共病/数据/multimorbidity_distribution_data.csv'\n",
    "output_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587ab9b",
   "metadata": {},
   "source": [
    "### 频数分布曲线告诉了我们ICU中的共病分布是遵循幂律分布的，具体来说它有3个特点：（1）在ICU中非常常见的共病占少数；（2）大多数共病的出现频次相对较低；（3）两图中的长尾部分阐明了在ICU中有大量的不常见但仍然存在的共病。\n",
    "### 对于ICU来说，这意味着：\n",
    "\n",
    "+ 对于最常见的那部分共病，ICU可能需要针对性的、高效的预防和治疗策略，因为这些常见的可能对大多数病人都很相关。\n",
    "\n",
    "+ 对于长尾部分的共病，即使它们相对少见，但因为种类繁多，总体上会影响到很多病人。这就需要ICU具备更广泛的专业知识和能力来处理这些相对罕见但种类繁多的情况。\n",
    "\n",
    "### 这种分布也强调了在医学研究和治疗策略制定中，既要重视高频发生的情况，也不能忽略那些低频但多样的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944cc6c6",
   "metadata": {},
   "source": [
    "# 共病网络由R语言构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4cc0f",
   "metadata": {},
   "source": [
    "### 首先构建疾病的共现矩阵，即基于同时出现的病例计数来构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd55b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make a list of all unique diseases across all patients\n",
    "all_diseases = set()\n",
    "for multimorbidity_list in aggregated_multimorbidity['multimorbidity_icd10']:\n",
    "    all_diseases.update(multimorbidity_list.split(', '))\n",
    "# Convert the set to a list to have a consistent order\n",
    "all_diseases = sorted(list(all_diseases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ad06cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize a co-occurrence matrix with zeros\n",
    "prevalence_matrix = pd.DataFrame(0.0, index=all_diseases, columns=all_diseases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6707462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a dictionary to hold the number of occurrences of each unique multimorbidity\n",
    "multimorbidity_occurrences = defaultdict(int)\n",
    "for multimorbidity in aggregated_multimorbidity['multimorbidity_icd10']:\n",
    "    # This creates a frozenset which is hashable and can be used as a dictionary key\n",
    "    diseases_set = frozenset(multimorbidity.split(', '))\n",
    "    multimorbidity_occurrences[diseases_set] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3309cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 321/321 [00:31<00:00, 10.05it/s]                                                            \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate the prevalence matrix\n",
    "# Note: Since the prevalence matrix is symmetric, we only need to calculate the upper triangle.\n",
    "for i, disease1 in tqdm(enumerate(prevalence_matrix.index), total=len(prevalence_matrix.index), bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}'):\n",
    "    for j, disease2 in enumerate(prevalence_matrix.columns[i+1:], start=i+1):  # Start from i+1 to avoid self-pairing and to calculate only the upper triangle\n",
    "        # Find all unique multimorbidities that contain both disease1 and disease2\n",
    "        M_ij = {m for m in multimorbidity_occurrences.keys() if disease1 in m and disease2 in m}\n",
    "        \n",
    "        # Calculate the sum of occurrences for all unique multimorbidities in M_ij\n",
    "        sum_occurrences = sum(multimorbidity_occurrences[m] for m in M_ij)\n",
    "        \n",
    "        # Calculate the prevalence for disease1, disease2 by dividing the sum of occurrences by the number of multimorbidities in M_ij\n",
    "        prevalence = sum_occurrences / len(M_ij) if M_ij else 0  # Avoid division by zero\n",
    "        \n",
    "        # Update the prevalence matrix for both (disease1, disease2) and (disease2, disease1) since it's symmetric\n",
    "        prevalence_matrix.at[disease1, disease2] = prevalence\n",
    "        prevalence_matrix.at[disease2, disease1] = prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a86a188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the prevalence matrix to a CSV file\n",
    "prevalence_matrix.to_csv('G:/共病/数据/prevalence_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c8f95",
   "metadata": {},
   "source": [
    "## 计算患者的共病严重程度，通过aggregated_multimorbidity表中的multimorbidity_icd10计算Elixhauser comorbidity score，Elixhauser comorbidity score是一个评估患者并发症严重程度的打分系统。正的得分越高可能意味着预后较差（负的得分越低意味着预后预后反而越好）。\n",
    "### 使用hcuppy计算Elixhauser comorbidity score中的mortality得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90487fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 321/321 [00:31<00:00, 10.03it/s]                                                            \n"
     ]
    }
   ],
   "source": [
    "ee = ElixhauserEngine()\n",
    "def calculate_elixhauser_scores(icd10_string):\n",
    "    # ICD codes were extracted, and ICD codes and descriptions were separated by ', '\n",
    "    icd_codes = icd10_string.split(', ')\n",
    "    # Eliminating empty strings\n",
    "    icd_codes = [code for code in icd_codes if code]\n",
    "    # Calculate the Elixhauser score\n",
    "    result = ee.get_elixhauser(icd_codes)\n",
    "    # readmission and mortality scores are returned\n",
    "    return pd.Series([result['rdmsn_scr'], result['mrtlt_scr']])\n",
    "\n",
    "# Create a dictionary to hold the Elixhauser mortality score for each unique multimorbidity\n",
    "multimorbidity_severity = defaultdict(int)\n",
    "for multimorbidity in aggregated_multimorbidity['multimorbidity_icd10']:\n",
    "    diseases_set = frozenset(multimorbidity.split(', '))\n",
    "    # Calculate the Elixhauser score for mortality\n",
    "    icd_codes = list(diseases_set)\n",
    "    result = ee.get_elixhauser(icd_codes)\n",
    "    mortality_score = result['mrtlt_scr']\n",
    "    # Store the mortality score for the multimorbidity\n",
    "    multimorbidity_severity[diseases_set] = mortality_score\n",
    "    \n",
    "# Initialize death score matrices\n",
    "severity_matrix = pd.DataFrame(\n",
    "    np.zeros_like(prevalence_matrix, dtype=float),\n",
    "    index=prevalence_matrix.index,\n",
    "    columns=prevalence_matrix.columns\n",
    ")\n",
    "\n",
    "# Iterate over each element of the severity_matrix matrix\n",
    "for i, disease1 in enumerate(tqdm(severity_matrix.index, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')):\n",
    "    for j, disease2 in enumerate(severity_matrix.columns[i+1:], start=i+1):  # Calculate only upper triangle\n",
    "        # Find all unique multimorbidities that contain both disease1 and disease2\n",
    "        M_ij = {m for m in multimorbidity_severity.keys() if disease1 in m and disease2 in m}\n",
    "        \n",
    "        # Calculate the sum of Elixhauser mortality scores for all unique multimorbidities in M_ij\n",
    "        sum_severity = sum(multimorbidity_severity[m] for m in M_ij)\n",
    "        \n",
    "        # Calculate the severity for disease1, disease2 by dividing the sum of scores by the number of multimorbidities in M_ij\n",
    "        severity = sum_severity / len(M_ij) if M_ij else 0  # Use np.nan for pairs with no multimorbidities\n",
    "        \n",
    "        # Update the severity matrix\n",
    "        severity_matrix.at[disease1, disease2] = severity\n",
    "        severity_matrix.at[disease2, disease1] = severity  # Symmetric update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "141d01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_matrix.to_csv('G:/共病/数据/severity_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8b710",
   "metadata": {},
   "source": [
    "### 根据final_chronic_diseases_dict对前面统计的aggregated_multimorbidity['multimorbidity_icd10']进行统计，对于每个共病m，通过dic计算每例患者共病包含几类不同的疾病组，将疾病组的数量作为共病复杂性评分C。然后通过co_occurrence_matrix构建complexity_matrix。对于complexity_matrix中的元素(i, j)，计算包含两种疾病i和疾病j的共病的平均复杂度评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b2435fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 321/321 [00:31<00:00, 10.33it/s]                                                            \n"
     ]
    }
   ],
   "source": [
    "complexity_matrix= pd.DataFrame(\n",
    "    np.zeros_like(co_occurrence_matrix, dtype=float),\n",
    "    index=co_occurrence_matrix.index,\n",
    "    columns=co_occurrence_matrix.columns\n",
    ")\n",
    "\n",
    "def calculate_complexity_scores(icd_codes, disease_categories):\n",
    "    # Create a mapping from ICD-10 code to Category\n",
    "    icd_to_category = disease_categories.set_index('ICD-10 Code')['Category'].to_dict()\n",
    "    # Find the unique categories for the multimorbidity's diseases\n",
    "    unique_categories = set(icd_to_category[icd] for icd in icd_codes if icd in icd_to_category)\n",
    "    # The complexity score is the number of unique categories\n",
    "    return len(unique_categories)\n",
    "\n",
    "dic = pd.read_excel('G:/final_chronic_diseases_dict.xlsx')\n",
    "\n",
    "# Calculate the complexity score for each multimorbidity\n",
    "multimorbidity_complexity = {}\n",
    "for multimorbidity in aggregated_multimorbidity['multimorbidity_icd10']:\n",
    "    icd_codes = multimorbidity.split(', ')\n",
    "    complexity_score = calculate_complexity_scores(icd_codes, dic)\n",
    "    diseases_set = frozenset(icd_codes)\n",
    "    multimorbidity_complexity[diseases_set] = complexity_score\n",
    "\n",
    "# Iterate over each element of the severity_matrix matrix\n",
    "for i, disease1 in enumerate(tqdm(complexity_matrix.index, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')):\n",
    "    for j, disease2 in enumerate(complexity_matrix.columns[i+1:], start=i+1):  # Calculate only upper triangle\n",
    "        # Find all unique multimorbidities that contain both disease1 and disease2\n",
    "        M_ij = {m for m in multimorbidity_complexity.keys() if disease1 in m and disease2 in m}\n",
    "        \n",
    "        # Calculate the sum of Elixhauser mortality scores for all unique multimorbidities in M_ij\n",
    "        sum_complexity = sum(multimorbidity_complexity[m] for m in M_ij)\n",
    "        \n",
    "        # Calculate the severity for disease1, disease2 by dividing the sum of scores by the number of multimorbidities in M_ij\n",
    "        complexity = sum_complexity / len(M_ij) if M_ij else 0  # Use np.nan for pairs with no multimorbidities\n",
    "        \n",
    "        # Update the severity matrix\n",
    "        complexity_matrix.at[disease1, disease2] = complexity\n",
    "        complexity_matrix.at[disease2, disease1] = complexity  # Symmetric update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f57371",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_matrix.to_csv('G:/共病/数据/complexity_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b79ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_array = complexity_matrix.to_numpy()\n",
    "severity_array = severity_matrix.to_numpy()\n",
    "prevalence_array = prevalence_matrix.to_numpy()\n",
    "multimorbidity_array = complexity_array * severity_array * prevalence_array\n",
    "multimorbidity_matrix = pd.DataFrame(multimorbidity_array,\n",
    "                              index=prevalence_matrix.index,\n",
    "                              columns=prevalence_matrix.columns)\n",
    "multimorbidity_matrix.to_csv('G:/共病/数据/multimorbidity_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2aa7fa",
   "metadata": {},
   "source": [
    "# 根据APs计算边的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518d275",
   "metadata": {},
   "source": [
    "### 首先对三个子矩阵的分布情况进行确认：如果数据呈正态分布，使用等距分箱法；如果数据呈现偏态分布，使用等频分箱法；如果数据呈长尾分布，使用基于分位数的分箱法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6119527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_path = 'G:/共病/图片/supplementary_fig1_severity_distribution.png'\n",
    "prevalence_path = 'G:/共病/图片/supplementary_fig1_prevalence_distribution.png'\n",
    "complexity_path = 'G:/共病/图片/supplementary_fig1_complexity_distribution.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6923303",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_flattened_data = severity_matrix.where(np.triu(np.ones(severity_matrix.shape), k=1).astype(bool)).stack().values\n",
    "severity_flattened_data_nonzero = severity_flattened_data[severity_flattened_data != 0]\n",
    "mu, std = stats.norm.fit(severity_flattened_data_nonzero)\n",
    "plt.figure(figsize=(2, 2))\n",
    "n, bins, patches = plt.hist(severity_flattened_data_nonzero, bins=30, color='#7362AC', edgecolor='#CECFE6', linewidth=0.25)\n",
    "\n",
    "\n",
    "# Adding a 'best fit' line\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=1)\n",
    "plt.title(title)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(severity_path, dpi=600, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a900f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_flattened_data = prevalence_matrix.where(np.triu(np.ones(prevalence_matrix.shape), k=1).astype(bool)).stack().values\n",
    "prevalence_flattened_data_nonzero = prevalence_flattened_data[prevalence_flattened_data != 0]\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.hist(prevalence_flattened_data_nonzero, bins=30, color='#E35508', edgecolor='#FCC38E', linewidth=0.25)\n",
    "plt.title('Distribution of Prevalence Matrix Elements')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(prevalence_path, dpi=600, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a52af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_flattened_data = complexity_matrix.where(np.triu(np.ones(complexity_matrix.shape), k=1).astype(bool)).stack().values\n",
    "complexity_flattened_data_nonzero = complexity_flattened_data[complexity_flattened_data != 0]\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.hist(complexity_flattened_data_nonzero, bins=30, color='#2E9750', edgecolor='#B7E3B2', linewidth=0.25)\n",
    "plt.title('Distribution of Complexity Matrix Elements')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(complexity_path, dpi=600, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c4f1e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-13.001, -7.0]      23\n",
       "(-7.0, -1.0]        341\n",
       "(-1.0, 5.0]        1275\n",
       "(5.0, 11.0]        3077\n",
       "(11.0, 17.0]       3464\n",
       "(17.0, 23.0]       2003\n",
       "(23.0, 29.0]        742\n",
       "(29.0, 35.0]        173\n",
       "(35.0, 41.0]         90\n",
       "(41.0, 47.0]         13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bins\n",
    "min_val = min(severity_flattened_data_nonzero)\n",
    "max_val = max(severity_flattened_data_nonzero)\n",
    "severity_bins = np.linspace(start=min_val, stop=max_val, num=11)\n",
    "severity_binned = pd.cut(severity_flattened_data_nonzero, bins=severity_bins, include_lowest=True)\n",
    "severity_binned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "daf849df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999, 1.016]    8521\n",
       "(1.016, 1.041]     330\n",
       "(1.041, 1.062]     349\n",
       "(1.062, 1.083]     308\n",
       "(1.083, 1.114]     325\n",
       "(1.114, 1.155]     327\n",
       "(1.155, 1.204]     327\n",
       "(1.204, 1.297]     328\n",
       "(1.297, 1.5]       395\n",
       "(1.5, 6.5]         261\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence_binned = pd.qcut(prevalence_flattened_data_nonzero, q=35, duplicates='drop')\n",
    "prevalence_binned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d62378d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 1: Range (1.0 - 2.857142857142857), Count: 1147\n",
      "[1.         1.         1.         ... 2.85714286 2.85714286 2.85714286]\n",
      "Bin 2: Range (2.857142857142857 - 3.111111111111111), Count: 1147\n",
      "[2.85714286 2.86013986 2.86075949 ... 3.11111111 3.11111111 3.11111111]\n",
      "Bin 3: Range (3.1153846153846154 - 3.6666666666666665), Count: 1147\n",
      "[3.11538462 3.11764706 3.11764706 ... 3.66666667 3.66666667 3.66666667]\n",
      "Bin 4: Range (3.6666666666666665 - 4.0), Count: 1147\n",
      "[3.66666667 3.66666667 3.66666667 ... 4.         4.         4.        ]\n",
      "Bin 5: Range (4.0 - 4.111111111111111), Count: 1147\n",
      "[4.         4.         4.         ... 4.11111111 4.11111111 4.11111111]\n",
      "Bin 6: Range (4.111111111111111 - 4.5), Count: 1147\n",
      "[4.11111111 4.11111111 4.11111111 ... 4.5        4.5        4.5       ]\n",
      "Bin 7: Range (4.5 - 4.847222222222222), Count: 1147\n",
      "[4.5        4.5        4.5        ... 4.84615385 4.84615385 4.84722222]\n",
      "Bin 8: Range (4.848484848484849 - 5.0), Count: 1147\n",
      "[4.84848485 4.85       4.85       ... 5.         5.         5.        ]\n",
      "Bin 9: Range (5.0 - 5.777777777777778), Count: 1147\n",
      "[5.         5.         5.         ... 5.77777778 5.77777778 5.77777778]\n",
      "Bin 10: Range (5.784810126582278 - 9.0), Count: 1148\n",
      "[5.78481013 5.78947368 5.8        ... 9.         9.         9.        ]\n"
     ]
    }
   ],
   "source": [
    "def equal_frequency_binning(data, num_bins):\n",
    "    # Sort the data\n",
    "    sorted_data = np.sort(data)\n",
    "    \n",
    "    # Calculate the number of data points per bin\n",
    "    bin_size = len(sorted_data) // num_bins\n",
    "    \n",
    "    # Create bins\n",
    "    bins = [sorted_data[i * bin_size: (i + 1) * bin_size] for i in range(num_bins)]\n",
    "    \n",
    "    # Handle the case where the number of data points is not perfectly divisible by num_bins\n",
    "    # by including the remainder in the last bin\n",
    "    remainder = len(sorted_data) % num_bins\n",
    "    if remainder > 0:\n",
    "        bins[-1] = np.concatenate((bins[-1], sorted_data[-remainder:]))\n",
    "    \n",
    "    return bins\n",
    "bins = equal_frequency_binning(complexity_flattened_data_nonzero, 10)\n",
    "\n",
    "# Output the bins\n",
    "for i, b in enumerate(bins):\n",
    "    print(f\"Bin {i+1}: Range ({b[0]} - {b[-1]}), Count: {len(b)}\")\n",
    "    print(b)  # Print the actual data in each bin (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "808835e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_intervals = pd.IntervalIndex.from_tuples([\n",
    "    (-13.001, -7.0), (-7.0, -1.0), (-1.0, 5.0), (5.0, 11.0),\n",
    "    (11.0, 17.0), (17.0, 23.0), (23.0, 29.0), (29.0, 35.0),\n",
    "    (35.0, 41.0), (41.0, 47.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f06a9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_intervals = pd.IntervalIndex.from_tuples([\n",
    "    (0.999, 1.016), (1.016, 1.041), (1.041, 1.062), (1.062, 1.083),\n",
    "    (1.083, 1.114), (1.114, 1.155), (1.155, 1.204), (1.204, 1.297),\n",
    "    (1.297, 1.5), (1.5, 6.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "979c501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_intervals = pd.IntervalIndex.from_tuples([\n",
    "    (0.99, 2.86), (2.86, 3.11), (3.11, 3.67), (3.67, 4.00),\n",
    "    (4.00, 4.11), (4.11, 4.50), (4.50, 4.85), (4.85, 5.00), \n",
    "    (5.00, 5.78), (5.78, 9.00)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e0ad967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_rating(value, intervals):\n",
    "    for i, interval in enumerate(intervals, start=1):\n",
    "        if interval.left <= value <= interval.right:\n",
    "            return i\n",
    "    return 0  # 如果值不在任何区间内\n",
    "def label_matrix(base_matrix, intervals):\n",
    "    labeled_matrix = pd.DataFrame(\n",
    "        np.zeros_like(base_matrix, dtype=float),\n",
    "        index=base_matrix.index,\n",
    "        columns=base_matrix.columns,\n",
    "    )\n",
    "    for i in base_matrix.index:\n",
    "        for j in base_matrix.columns:\n",
    "            if base_matrix.loc[i, j] != 0:\n",
    "                labeled_matrix.loc[i, j] = assign_rating(base_matrix.loc[i, j], intervals)\n",
    "    return labeled_matrix\n",
    "\n",
    "labeled_severity_matrix = label_matrix(severity_matrix, severity_intervals)\n",
    "labeled_prevalence_matrix = label_matrix(prevalence_matrix, prevalence_intervals)\n",
    "labeled_complexity_matrix = label_matrix(complexity_matrix, complexity_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3ca2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_ap(severity, prevalence, complexity):\n",
    "    if severity >= 9:\n",
    "        if prevalence >= 8 or complexity >= 7:\n",
    "            return 'H'\n",
    "        else:\n",
    "            return 'M' if complexity == 1 else 'L'\n",
    "    elif severity >= 7:\n",
    "        if prevalence >= 8 or complexity >= 7:\n",
    "            return 'H'\n",
    "        else:\n",
    "            return 'M' if complexity <= 4 else 'L'\n",
    "    elif severity >= 4:\n",
    "        if prevalence >= 8 or complexity >= 5:\n",
    "            return 'H'\n",
    "        elif complexity >= 2:\n",
    "            return 'M'\n",
    "        else:\n",
    "            return 'L'\n",
    "    elif severity >= 2:\n",
    "        if prevalence >= 8 and complexity >= 7:\n",
    "            return 'M'\n",
    "        else:\n",
    "            return 'L'\n",
    "    else:\n",
    "        return 'L'\n",
    "def calculate_ap_matrix(severity_matrix, prevalence_matrix, complexity_matrix):\n",
    "    ap_matrix = pd.DataFrame(\n",
    "        np.empty(severity_matrix.shape, dtype=str),\n",
    "        index=severity_matrix.index,\n",
    "        columns=severity_matrix.columns\n",
    "    )\n",
    "    for i in severity_matrix.index:\n",
    "        for j in severity_matrix.columns:\n",
    "            severity_rating = severity_matrix.loc[i, j]\n",
    "            prevalence_rating = prevalence_matrix.loc[i, j]\n",
    "            complexity_rating = complexity_matrix.loc[i, j]\n",
    "            ap_matrix.loc[i, j] = determine_ap(severity_rating, prevalence_rating, complexity_rating)\n",
    "    return ap_matrix\n",
    "ap_matrix = calculate_ap_matrix(labeled_severity_matrix, labeled_prevalence_matrix, labeled_complexity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db22f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_matrix.to_csv('G:/共病/数据/ap_matrix.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
